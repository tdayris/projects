import sys
import pandas
import os.path

from snakemake.utils import min_version, makedirs
from pathlib import Path
from typing import Any, Dict

if sys.version_info < (3, 8):
    raise SystemError("Please use Python 3.8 or later")

min_version('5.16.0')
git = "https://raw.githubusercontent.com/tdayris/snakemake-wrappers/Unofficial"
container: "docker://continuumio/miniconda3:5.0.1"


compressed_vcf = Path(
    "/mnt/beegfs/scratch/bioinfo_core/B19111_ETRO_01/annovar/snpsift/GeneSets/"
)

vcf_paths = {
    str(f.name)[:-7]: f
    for f in compressed_vcf.iterdir()
    if str(f).endswith(".vcf.gz")
}

rule all:
    input:
        "table/result.tsv"
    message:
        "Finishing pipeline"


rule tabix_index:
    input:
        "{file}"
    output:
        "{file}.tbi"
    message:
        "Indexing {wildcards.file}"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    params:
        "-p vcf"
    log:
        "logs/tabix{file}.log"
    wrapper:
        f"{git}/bio/tabix"


rule snpsift_filter:
    input:
        vcf = lambda w: str(vcf_paths[w.sample]),
        idx = lambda w: f"{vcf_paths[w.sample]}.tbi"
    output:
        vcf = temp("snpsift/filtered/{sample}.vcf.gz")
    message:
        "Filtering {wildcards.sample}"
    threads:
        4
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    params:
        snpsift_filters = "((! exists dbNSFP_ExAC_NFE_AF) | (dbNSFP_ExAC_NFE_AF <= 0.01))",
        bcftools_filter = "FORMAT/AD>5",
        view_filters = "-M 1"
    conda:
        "../../envs/biotools.yaml"
    log:
        view = "logs/bcftools/view/{sample}.log",
        bcftools_filter = "logs/bcftools/filter/{sample}.log",
        snpsift_filter = "logs/snpsift/filter/{sample}.log",
        bgzip = "logs/bgzip/filter/{sample}.log"
    shell:
        " bcftools filter "
        " --include '{params.bcftools_filter}' "
        " --output-type v "
        " {input.vcf} "
        " 2> {log.bcftools_filter} "
        " | "
        "  SnpSift filter "
        "'{params.snpsift_filters}' "
        " 2> {log.snpsift_filter} "
        " | "
        " bgzip -c > {output.vcf} "
        "2> {log.bgzip}"


rule snpsift_vartype:
    input:
        vcf = "snpsift/filtered/{sample}.vcf.gz"
    output:
        vcf = temp("snpsift/varType/{sample}.vcf")
    message:
        "Computing variant type for {wildcards.sample}"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    group:
        "split_variants"
    log:
        "logs/snpsift/vartype/{sample}.log"
    wrapper:
        f"{git}/bio/snpsift/varType"


FIELDS = (
    "CHROM "                             # 0
    "POS "
    "REF "
    "ALT "
    "FILTER "
    "VAF "                              # 5
    "NAD "
    "NRD "
    "NDP "
    "ANN[*].EFFECT "
    "ANN[*].GENE "                      # 10
    "ANN[*].GENEID "
    "ANN[*].IMPACT "
    "ANN[*].HGVS_P "
    "ANN[*].HGVS_C "
    "ANN[*].CDNA_POS "                  # 15
    "ANN[*].BIOTYPE "
    "dbNSFP_Interpro_domain "
    "dbNSFP_1000Gp3_EUR_AC "
    "dbNSFP_Uniprot_acc "
    "dbNSFP_SIFT_pred "                 # 20
    "dbNSFP_MetaSVM_pred "
    "dbNSFP_MutationTaster_pred "
    "dbNSFP_MutationAssessor_pred "
    "dbNSFP_PROVEAN_pred "
    "dbNSFP_LRT_pred "                  # 25
    "dbNSFP_Polyphen2_HDIV_pred "
    "dbNSFP_FATHMM_pred "
    "GWASCAT_PUBMED_ID "
    "MSigDb "
    "dbNSFP_ExAC_NFE_AF "               # 30
    "dbNSFP_ExAC_SAS_AF "
    "dbNSFP_ExAC_Adj_AF "
    "dbNSFP_ExAC_AFR_AF "
    "dbNSFP_ExAC_AF "
    "dbNSFP_ExAC_FIN_AF "               # 35
    "dbNSFP_1000Gp3_EUR_AF "
    "dbNSFP_ExAC_AMR_AF "
    "dbNSFP_ExAC_EAS_AF "
    "dbNSFP_ESP6500_EA_AF "
    "dbNSFP_1000Gp3_AMR_AF "            # 40
    "dbNSFP_1000Gp3_AF "
    "dbNSFP_1000Gp3_EAS_AF "
    "dbNSFP_1000Gp3_EUR_AF "
    "dbNSFP_1000Gp3_AFR_AF "
    "dbNSFP_1000Gp3_SAS_AF"             # 45
)

rule compress_and_index:
    input:
        "snpsift/varType/{sample}.vcf"
    output:
        vcf = "snpsift/varType/{sample}.vcf.gz",
        tbi = "snpsift/varType/{sample}.vcf.gz.tbi"
    message:
        "Compressing and indexing {wildcards.sample}"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    group:
        "format"
    log:
        "logs/compress_and_index/{sample}.log"
    conda:
        "../../envs/biotools.yaml"
    shell:
        "bgzip -c {input} > {output.vcf} && tabix -p vcf {output.vcf}"

rule calculate_vaf:
    input:
        "snpsift/varType/{sample}.vcf.gz"
    output:
        temp("snpsift/vafed/{sample}.vcf.gz")
    message:
        "Filtering on VAF"
    threads:
        2
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    group:
        "format"
    # conda:
    #     "../../envs/py37.yaml"
    log:
        "logs/calculate_vaf/{sample}.log"
    run:
        import gzip
        never = True

        with gzip.open(input[0], "rb") as vcf, gzip.open(output[0], "w") as out:
            for line in vcf:
                if line.startswith(b"#"):
                    out.write(line)
                    if never is True:
                        out.write(b"##INFO=<ID=VAF,Number=A,Type=Float,Description=\"Variant Allele frequency\">\n")
                        out.write(b"##INFO=<ID=NAD,Number=A,Type=Float,Description=\"Variant allele depth\">\n")
                        out.write(b"##INFO=<ID=NDP,Number=A,Type=Float,Description=\"Read depth\">\n")
                        out.write(b"##INFO=<ID=NRD,Number=A,Type=Float,Description=\"Reference allele depth\">\n")
                        never = False
                    continue

                chomp = line[:-1].split(b"\t")
                try:
                    sample = {
                        f: s
                        for f, s in zip(
                            chomp[8].split(b":"),
                            chomp[9].split(b":")
                        )
                    }
                    vaf = int(sample[b"AD"].split(b",")[1]) / int(sample[b"DP"])
                    if vaf > 0.1:
                        chomp[7] += b";VAF=" + str(vaf).encode()
                        chomp[7] += b";NAD=" + sample[b"AD"]
                        chomp[7] += b";NDP=" + sample[b"DP"]
                        chomp[7] += b";NRD=" + str(int(sample[b"DP"]) - int(sample[b"AD"].split(b",")[1])).encode()
                        out.write(b"\t".join(chomp) + b"\n")

                except KeyError:
                    pass
                except ZeroDivisionError:
                    pass



rule snpsift_table:
    input:
        vcf = "snpsift/vafed/{sample}.vcf.gz"
    output:
        tsv = "snpsfit/table/{sample}.tsv"
    message:
        "Building table from vcf file for {wildcards.sample}"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/biotools.yaml"
    group:
        "format"
    params:
        fields = FIELDS
    log:
        "logs/snpsift_table/{sample}.log"
    shell:
        "SnpSift extractFields "
        " -e '.' "
        " -s ',' "
        " {input.vcf} "
        " {params.fields} "
        " > {output.tsv} "
        " 2> {log}"


rule add_sample_name:
    input:
        tsv = "snpsfit/table/{sample}.tsv"
    output:
        tsv = "table/samples/{sample}.tsv"
    message:
        "Adding sample name to result table for {wildcards.sample}"
    threads:
        2
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    log:
        "logs/add_sample_name/{sample}.log",

    shell:
        "awk 'BEGIN{{FS=\"\\t\"}} "
        "{{if (NR==1) {{print \"SAMPLE\" FS $0}} "
        "else {{print \"{wildcards.sample}\" FS $0}} }}' "
        "{input.tsv} > {output.tsv} 2> {log}"


rule concatenate:
    input:
        expand("table/samples/{sample}.tsv", sample=vcf_paths.keys())
    output:
        "tables/complete.tsv"
    message:
        "TSV concatenation"
    threads:
        5
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    group:
        "concatenate"
    log:
        head = "logs/concatenate/head.log",
        sed = "logs/concatenate/sed.log",
        sort = "logs/concatenate/sort.log",
        uniq = "logs/concatenate/uniq.log"
    shell:
        "head -n 1 {input[0]} > {output} 2> {log.head} && "
        "for TSV in {input}; do sed '1d' ${{TSV}} 2> {log.sed}; done "
        " | sort 2> {log.sort} | uniq >> {output} 2> {log.uniq}"


rule variant_frequency:
    input:
        tsv = "tables/complete.tsv"
    output:
        tsv = temp("table/frequency.tsv")
    message:
        "Computing variant grequency among population"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: attempt * 4096
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/python3.8.yaml"
    params:
        chr = "CHROM",
        pos = "POS",
        alt = "ALT"
    log:
        "logs/variant_frequency.log"
    script:
        "../../scripts/variant_frequency.py"


rule bash_variant_frequency:
    input:
        tsv = "tables/complete.tsv"
    output:
        tsv = "tables/frequencies.tsv"
    message:
        "Building frquencies table"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    log:
        "logs/bash_variant_frequency.log"
    shell:
        "cut -d '\\t' -f "


rule filter_out_frequency:
    input:
        "table/frequency.tsv"
    output:
        temp("table/filtered/frequency.tsv")
    message:
        "Filtering on populaiton frequency on final table"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: attempt * 4096
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    params:
        freq = "< 5"
    group:
        "awk_filter"
    log:
        "logs/filter/frequency.log"
    shell:
        "awk 'BEGIN{{FS=\"\\t\"}}"
        " {{if ($2 {params.freq}) {{print $0}} }}"
        " {input} > {output} 2> {log}"


rule filter_out_vaf:
    input:
        "table/filtered/frequency.tsv"
    output:
        temp("table/filtered/vaf.tsv")
    message:
        "Filtering on VAF"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    params:
        vaf = ">= 0.3"
    group:
        "awk_filter"
    log:
        "logs/filter/vaf.log"
    shell:
        "awk 'BEGIN{{FS=\"\\t\"}}"
        " {{if ($8 {params.vaf}) {{print $0}} }}"
        " {input} > {output} 2> {log}"


rule expand_variants:
    input:
        "table/filtered/vaf.tsv"
    output:
        temp("table/filtered/expanded.tsv")
    message:
        "Expanding variants"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/python3.8.yaml"
    log:
        "logs/expand_variants.log"
    script:
        "expand_variants.py"


rule filter_out_utr:
    input:
        "table/filtered/expanded.tsv"
    output:
        temp("table/filtered/utr.tsv")
    message:
        "Filtering out UTR"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    log:
        "logs/filter/utr.log"
    shell:
        "grep -vP \"\\tUTR_\" {input} > {output} 2> {log}"


rule filter_gene_name:
    input:
        no_utr = "table/filtered/utr.tsv",
        gene_list = "gene_list.txt"
    output:
        temp("table/filtered/gene_name.tsv")
    message:
        "Filter on gene names"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    log:
        "logs/filter/gene_name.log"
    shell:
        "grep --file {input.gene_list} {input.no_utr} > {output} 2> {log}"


rule drop_duplicates:
    input:
        "table/filtered/gene_name.tsv"
    output:
        "table/result.tsv"
    message:
        "Filtering out duplicates"
    threads:
        1
    resources:
        mem_mb = (
            lambda wildcards, attempt: min(attempt * 1024, 10240)
        ),
        time_min = (
            lambda wildcards, attempt: min(attempt * 20, 200)
        )
    conda:
        "../../envs/bash.yaml"
    log:
        "logs/filter/duplicates.log"
    shell:
        "uniq {input} > {output} 2> {log}"
